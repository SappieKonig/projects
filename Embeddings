Embeddings are very popular in Machine Learning (ML). Embeddings can be seen as a compression of data, a compression that is quite possibly easier to interpret than the original data.

To the best of my knowledge, this idea forms the basis of all unsupervised learning techniques. Embeddings have the function of filteriong out unnecessary garbage that is statistically insignificant. Or in other words, embeddings try to filter out the relevant information in the image.

This has led to frameworks that try to map perturbations of an image to the same embedding, achieving fine results. But let's compare this to supervised learning. The supervised learning that I'll discuss tries to map an image input to a certain output. Unsupervised learning, by its promise, needs only the images. The promise of unsupervised learning then, is that one can get away with training on unlabeled data.

But what do you train when you train a network in an unsupervised way? As said before, you map images to embeddings, after which you stack a linear classifier on top of that. How does this differ from supervised learning? A network with N layers trained in a supervised way can be seen as an embedding network of N-1 layers and a linear classifier on top of that. So the difference is the way you train the features in the embedding layer. You want features that are useful for CLASSIFICATION. So optimizing unsupervised learning frameworks is part genuine technological improvement (preventing embedding collapse), but also for a big part optimizing embeddings so that you get features that are useful for CLASSIFICATION. In other words, the process of research of unsupervised learning can partly be seen as overfitting to a certain task, invalidating the research at least in part.

Embeddings filter out irrelevant information. And as noted before, what is seen as irrelevant is task dependant. In other words, we want GOAL ORIENTED EMBEDDINGS, that is, a way to dynamically filter out information according to the task at hand. This definitely feels like a catch-22 (and maybe it is), because without performing the task, how do you know what information is important to the task? The classic machine learning approach would be to embed these tasks, place them on a continuum by function approximation, train on thousands if not millions of tasks, and call it a day. But this is hardly efficient or even feasible. So we've kicked the can down the road. Although, let's say this works, out comes an interesting property. If we can place tasks on a continuous function, there is a chance we immediately solve the problem of sparse rewards. We just look at the goal's place on the continuum, compare it to our current position and move towards the position of our goal.

DISCLAIMER: the above paragraph uses the embedding of the goal and the embedding of the state interchangeably, which is, of course, a fallacy. 

Proposed efficient way of putting the goal on a continuum --> decide the goal after you do something! Wow, that's like the secret to a happy life or something. Do something in your stupid simulated world, see what happens, call it a goal completed! (Yes my standards for happiness are low, shut up ;)) And then you have a free point for your continuum! Small problem, I currently have exactly zero ways to force this continuum to be fancy or smooth or consistent or representative or useful but shhhhhhhhhhhhhhhh.

END RANT FOR NOW
