General outline idea 1:
    - make architecture as general as possible (Dense vs Transformer)
    - generality comes at cost of more computation, solution --> pruning
        --> problem: current pruning sucks, gotta find something that's better than setting random weight to zero
            idea: use NN's! (graph NN, or simple Dense over input, weight and output)
                assumptions:
                    - there is a general pattern between neurons and their connections, regardless of task (supported by paper: general computation blablabla)
                    - NN can approximate at least current techniques (100% true, current techniques suck)

                hopes:
                    - NN slow, so maybe it condenses to a function that can be approximated algorithmically (simple formula)
                    - reversible, so you can start with a very sparse network and build and shave from that (more like human brain, speeds up even training and decreases memory footprint during training)
                    - what you want three? the above two aren't paper worthy on their own?

    - increased generality, even from transformers?
        what assumptions do transformers still make about their data?
            --> possibly the seperation of impact on itself and impact on other tokens? would a fusion be better?

        hypothesis:
            they are absolutely fine for now

    SO WE WANT A TRANSFORMER THAT IS BUILT FROM SCRATCH USING REVERSE PRUNING, GOAL NUMBER ONE



General outline idea 2:
    - create meaning within a network --> clip is amazing.
    - problem?
        --> gpt3 might have the problem that it has only seen the abstract form of language, language is a derivative of the world around us, an abstract representation of reality. Hopes: clip --> clip connects text and vision, abstract and concrete. Hence, clip might very well be on the way towards intelligence.
    - question?
        --> does multi-modality automatically increase understanding? would gpt3 become smarter by feeding it videos alongside text? (temporally correlated images).
    - test?
        --> why not just copy a human, and see how our NN holds? As in put an NN in a human environment, or at least let it use the same senses in a simulated / web-scraped environment
    - DO NOT HAVE THAT KIND OF COMPUTE, NOT A VIABLE OPTION AS OF YET, JUST WANNA SEE WHAT HAPPENS



General outline idea 3:
    - curriculum! What speedups can curricula give us? Should be quite a bit if they kick as much ass as I hope them to
    - AGAIN, NOT AS VIABLE ON SMALL SCALE, SO EITHER GOTTA FIND A WAY TO MINIMIZE IT OR HACK GOOGLE'S DATACENTERS



General outline idea 4:
    - consensus labelling --> label = what most networks think it is. Goal? To train with as little given labels as possible. Ensemble, yes, but ensembles are too interesting to just ignore --> downsides, from what I know, already heavily explored



General outline idea 5:
    - autoencoding through multitude of text-image models as a way of creating captions on an image.
    - many language models just talking with one another, see what happens
        --> problem? NNs don't work without a goal. So how about curiosity? Try to get the other network to say words you haven't often heard whilst maximising the probability other networks agree with both you and your conversation partner
            --> goal: interesting conversation, whilst still being logical (hence checked by others)
                --> cooler goal: see what the heck language models can do (still think text-image is essential, maybe even text-video)



General outline idea 6:
    - weights are practically linear correlation coefficients between different neurons. So could we train them as correlations? Increase the weight in the direction of the correlation between neurons if the netwerk did good, decrease it otherwise.
    - benefits:
        1. no need for gradients
        2. direct training on advantages



General outline idea 7:
    - generalization of Dream oder --> NNs as functions
    - things to look at: 
        1. Function parsing --> breaking NN into multiple blocks
           Idea: data goes through singular layers where the flow is controlled by a controller network. 
                 These smaller networks serve as function. Loss that minimizes the amount of layers used? Sort of line minimalizer.
        2. Output generality: for a network to be general its output should be general as well. How can one output be interpreted by multiple tasks? 
           Idea: text --> input to general output to task specific output.
        3. Curriculum --> this one is difficult.



General outline idea 8:
  - unsupervised learning through GPT style training
  - contrastive loss between model(x) and model(gpt(x))
  - still only enforces coherence of position, making it quite fucking limited
